---
title: "Bayesian hierarchical modeling"
author: "Jingchen (Monika) Hu"
date: "MATH 347 Bayesian Statistics"
output:
  pdf_document: default
  html_document:
    number_sections: yes
institute: Vassar College
fontsize: 11pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
require(runjags)
require(coda)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Installing the necessary packages

```{r, eval = FALSE}
#install.packages("devtools")
require(devtools)
devtools::install_github("bayesball/ProbBayes")

require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
```



# Example: Korean Drama Ratings

## Ratings by Schedule

```{r message = FALSE}
dramadata = read.csv("KDramaData.csv", header=T)

KBSdrama = dramadata[dramadata$Producer==2,]
KBSdrama$Schedule = as.factor(KBSdrama$Schedule)
```

```{r echo = FALSE}
ggplot(KBSdrama, aes(Rating, color = Schedule)) +
  geom_density() + labs(title = "Density plot of ratings") + 
  xlim(0, 0.3) + theme_grey(base_size = 10, base_family = "") 
```


```{r}
table(KBSdrama$Schedule)
tapply(KBSdrama$Rating, KBSdrama$Schedule, summary)
tapply(KBSdrama$Rating, KBSdrama$Schedule, sd)
```

# Observations in groups: approaches to modeling

# A two-stage prior in a hierarchical model

## The Hierarchical Normal Model

-  The sampling density for group $j$, and $j = 1, \cdots, J$:
\begin{eqnarray}
Y_{ij} \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, {\color{red}\sigma}),
\end{eqnarray}
where $i = 1, \cdots, n_j$ and $n_j$ is the number of observations in group $j$. 

- The stage 1 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu_j \sim \textrm{Normal}(\mu, \tau).
\end{eqnarray}

- The stage 2 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu, \tau \sim \textrm{g}(\mu, \tau).
\end{eqnarray}

- The prior distribution for $\sigma$:
\begin{eqnarray}
1/\sigma^2 \sim \textrm{Gamma}(\alpha_{\sigma}, \beta_{\sigma}).
\end{eqnarray}



## Prior and Hyperprior Specifications

- The stage 1 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu_j \sim \textrm{Normal}(\mu, \tau).
\end{eqnarray}

- The stage 2 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu, \tau \sim \textrm{g}(\mu, \tau).
\end{eqnarray}

- Hyperpriors:
\begin{eqnarray}
\mu \mid \mu_0, \gamma_0 &\sim& \textrm{Normal}(\mu_0, \gamma_0),\\
1/\tau^2 \mid \alpha_{\tau}, \beta_{\tau} &\sim& \textrm{Gamma}(\alpha_{\tau}, \beta_{\tau}).
\end{eqnarray}


- The prior distribution for $\sigma$:
\begin{eqnarray}
1/\sigma^2 \sim \textrm{Gamma}(\alpha_{\sigma}, \beta_{\sigma}).
\end{eqnarray}


# MCMC simulation by JAGS

## Recap: Prior and Hyperprior Specifications

- The stage 1 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu_j \sim \textrm{Normal}(\mu, \tau).
\end{eqnarray}

- The stage 2 prior distribution for $\mu_j$:
\begin{eqnarray}
\mu, \tau \sim \textrm{g}(\mu, \tau).
\end{eqnarray}

- Hyperpriors:
\begin{eqnarray}
\mu \mid \mu_0, \gamma_0 &\sim& \textrm{Normal}(0.1, 0.5),\\
1/\tau^2 \mid \alpha_{\tau}, \beta_{\tau} &\sim& \textrm{Gamma}(1, 1).
\end{eqnarray}

- The prior distribution for $\sigma$:
\begin{eqnarray}
1/\sigma^2 \sim \textrm{Gamma}(1, 1).
\end{eqnarray}

## JAGS Script for the Hierarchical Model

```{r message = FALSE}
modelString <-"
model {
## likelihood
for (i in 1:N){
y[i] ~ dnorm(mu_j[schedule[i]], invsigma2)
}

## priors
for (j in 1:J){
mu_j[j] ~ dnorm(mu, invtau2)
}
invsigma2 ~ dgamma(a_g, b_g)
sigma <- sqrt(pow(invsigma2, -1))

## hyperpriors
mu ~ dnorm(mu0, 1/g0^2)
invtau2 ~ dgamma(a_t, b_t)
tau <- sqrt(pow(invtau2, -1))
}
"
```

Note that `mu_j[schedule[i]]`, we need to determine which j this observation i belongs to. By passing the schedule of each observation as a vector, we can extract which $\mu_j$ that we should be working on.


- Notes about the \texttt{modelString}
    1. Need a vector of \texttt{mu\_j}, of length \texttt{J}.
    2. Need a vector of \texttt{schedule}, of length \texttt{N}.
    3. \texttt{dnorm} takes mean and \textcolor{red}{precision}.
    4. Work with \texttt{invsigma2}, can return \texttt{sigma}.
    5. Work with \texttt{invtau2}, can return \texttt{tau}.

- Pass the data and hyperparameter values to JAGS:

```{r message = FALSE}
y = KBSdrama$Rating   
schedule = KBSdrama$Schedule  
N = length(y)  
J = length(unique(schedule)) 

initsfunction <- function(chain){
  .RNG.seed <- c(1,2)[chain]
  .RNG.name <- c("base::Super-Duper",
                 "base::Wichmann-Hill")[chain]
  return(list(.RNG.seed=.RNG.seed,
              .RNG.name=.RNG.name))
}

the_data <- list("y" = y, "schedule" = schedule, "N" = N, "J" = J, 
                 "mu0" = 0.1, "g0" = 0.5, 
                 "a_t" = 1, "b_t" = 1,
                 "a_g" = 1, "b_g" = 1)
```

The `initsfunction` shows the way to set seed with JAGS. `set.seed()` won't work in JAGS.


- Run the JAGS code for this model:

```{r message = FALSE, warning = FALSE}
posterior <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("mu", "tau", "mu_j", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000,
                      thin = 1, 
                      inits = initsfunction)
```

`moniter` is the list of the parameters you want to track.

`adapt` has default 1000, you can think in a way that it's similar to `burnin`.

`burnin` is the number of draws that you want to toss out.

`sample` is the final number of iterations that you want to have.

`inits` is for reproducible setup.

## JAGS Output of the Hierarchical Model

- Obtain posterior summaries of all parameters:

\vspace{3mm}

```{r message = FALSE, warning = FALSE}
summary(posterior) 
```

Do you think the $\mu_j$'s (average ratings) do differ a lot across different groups? Look at the mean, they are different.

All credible intervals for $\mu_j$ contain 0, which means that they are not statistically significant from 0. 

Yet, the lower bounds of credible intervals for $mu_j$ and $\mu$ are negative. This doesn't make sense as the rating can not be negative. Using normal may not be suitable for this particular data type. Consider truncated normal.

```{r message = FALSE}
plot(posterior, vars = "mu_j[1]")
```


```{r message = FALSE}
plot(posterior, vars = "tau")
```

## Shrinkage/Pooling Effects

```{r message = FALSE, warning = FALSE}
Ind_Stats = as.data.frame(matrix(NA, J, 2))
names(Ind_Stats) = c("mean", "sd")
for (j in 1:J){
  Ind_Stats[j, ] = c(mean(KBSdrama$Rating[KBSdrama$Schedule == j]), 
                     sd(KBSdrama$Rating[KBSdrama$Schedule == j]))
}

Post_Means <- summary(posterior)[, 4]

Means1 <- data.frame(Type = "Sample", Mean = Ind_Stats$mean)
Means2 <- data.frame(Type = "Hierarchical", Mean =
                       Post_Means[3:(4 + J - 2)])

Means1$Title <- c("Schedule 1", "Schedule 2", "Schedule 3",
                  "Schedule 4")
Means2$Title <- c("Schedule 1", "Schedule 2", "Schedule 3",
                  "Schedule 4")
```


```{r message = FALSE}
ggplot(rbind(Means1, Means2), aes(Type, Mean, group=Title)) +
  geom_line(color = crcblue) + geom_point() +
  annotate(geom = "text", x = 0.75,
           y = Means1$Mean + c(0.01, 0.01, 0.01, -0.01), 
           size = 3, label = Means1$Title) + increasefont(Size = 10)
```

The right-hand side is the sample mean of the 4 different schedules.
On the left-hand side, we have the posterior mean after fitting the hierarchical model.


## Sources of Variability

- Two sources of variability in $Y_{ij}$:
\begin{eqnarray}
Y_{ij} &\overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma) \,\,\, \text{[within-group variability]}\\
\mu_j \mid \mu, \tau &\sim \textrm{Normal}(\mu, \tau) \,\,\, \text{[between-group variability]} 
\end{eqnarray} 


- To compare these two sources of variability, one can compute the fraction
\begin{eqnarray}
R = \frac{\tau^2}{\tau^2 + \sigma^2},
\end{eqnarray}
from the posterior draws of $\tau$ and $\sigma$.


- The closer the value of $R$ to 1, the higher the between-group variability.
## Compute and Graph Sources of Variability

- We need the \texttt{coda} R package

```{r eval = FALSE}
#install.packages("coda")
```

```{r message = FALSE, warning = FALSE}
require(coda)
tau_draws <- as.mcmc(posterior, vars = "tau")
sigma_draws <- as.mcmc(posterior, vars = "sigma")
R <- tau_draws^2/(tau_draws^2 + sigma_draws^2)

df <- as.data.frame(R)

quantile(R, c(0.025, 0.975))
```


```{r message = FALSE}
ggplot(df, aes(x=R)) + geom_density() +
  labs(title="Density of R") +
  theme(plot.title = element_text(size=15)) +
  theme(axis.title = element_text(size=15))
```


# Exercise: Hierarchical model with schedule-specific $\mu_j$ and $\sigma_j$

## P.37/42

- Specify a shared Normal prior distribution for $\mu_j$

\begin{align}
\mu_j\sim Normal(\mu, \tau)
\end{align}

We think the $\mu_j$'s are related to each other, but they are not the same. We think all of them come from normal distribution with mean $\mu$ (they should be somewhere around $\mu$ basically). $\tau$ controls how $\mu_j$ are going to be different from $\mu$ or different from each other.

- In addition, specify a shared Gamma prior distribution for $\sigma_j$

\begin{align}
\frac{1}{\sigma_j^2}\sim Gamma(s_a, s_b)
\end{align}

where the mean is $s_a/s_b$ and the variance is $s_a/s_b^2$

All of the $\sigma_j$'s are related somehow. The mean controls on average how they should be. The variance controls the similarity between the $\sigma_j$'s.


```{r message = FALSE}
modelString <-"
model {
## likelihood
for (i in 1:N){
y[i] ~ dnorm(mu_j[schedule[i]], invsigma2_j[schedule[i]])
}

## priors
for (j in 1:J){
mu_j[j] ~ dnorm(mu, invtau2)
invsigma2_j[j] ~ dgamma(s_a, s_b)
}

## sigma <- sqrt(pow(invsigma2, -1))

## hyperpriors need to add s_a and s_b as well
mu ~ dnorm(mu0, 1/g0^2)
invtau2 ~ dgamma(a_t, b_t)
tau <- sqrt(pow(invtau2, -1))
## s_a
## s_b
}
"
```

# Recap
