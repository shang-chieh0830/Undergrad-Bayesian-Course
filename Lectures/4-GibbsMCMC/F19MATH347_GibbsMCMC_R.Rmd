---
title: "Gibbs sampler and MCMC (R scripts)"
author: "Jingchen (Monika) Hu"
date: "MATH 347 Bayesian Statistics"
output:
  pdf_document: default
  html_document:
    number_sections: yes
institute: Vassar College
fontsize: 11pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Installing the necessary packages

```{r, eval = FALSE}
install.packages("devtools")
require(devtools)
devtools::install_github("bayesball/ProbBayes")

require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
```

# Example: Expenditures in the Consumer Expenditure Surveys

## The TOTEXPPQ variable in the CE sample

```{r message = FALSE}
CEsample <- read_csv("CEsample1.csv")

summary(CEsample$TotalExpLastQ)
sd(CEsample$TotalExpLastQ)
```

```{r fig.align = "center"}
ggplot(data = CEsample, aes(TotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q") +
  theme_grey(base_size = 8, base_family = "") 
```

## Log transformation of the TOTEXPPQ variable

```{r fig.align = "center"}
CEsample$LogTotalExpLastQ <- log(CEsample$TotalExpLastQ)
ggplot(data = CEsample, aes(LogTotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q (log)") +
  theme_grey(base_size = 8, base_family = "") 
```


## The Normal distribution

```{r fig.align = "center"}
ggplot(data = data.frame(y = c(-5, 5)), aes(y)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5), aes(color = "Normal(0, 0.5)")) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = "Normal(0, 1)")) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = "Normal(0, 2)")) +
  stat_function(fun = dnorm, args = list(mean = -2, sd = 0.5), aes(color = "Normal(-2, 0.5)")) +
  ylab("f(y)")
```



# Prior and posterior distributions for mean AND standard deviation

## Some derivations

Notice that we have unknown $\mu$ and $\sigma$ here. They are **both** unknown.
And we derive the full conditional posterior distributions (conditioning on all of the data and every other parameters, even though they are unknown).

e.g.

- $\mu|y_1,...,y_n,\phi$

- $\phi|y_1,...,y_n,\mu$


Assuming independence, joint density = product of marginal density


\begin{align}
\pi(\mu, \sigma)&=\pi_1(\mu)\pi_2(\sigma)\\
L(\mu, \sigma)&=f(y_1,...,y_n| \mu, \sigma)\\
\implies \pi(\mu,\sigma|y_1,...,y_n) &\propto \pi(\mu, \sigma)L(\mu, \sigma)
\end{align}

Then we need to derive the probability distribution of $\mu|y_1,..,y_n,\phi$ and $\phi|y_1,...,y_n,\mu$


\begin{align}
\pi(\mu, \sigma)&=\pi_1(\mu)\pi_2(\sigma)\\
&=\frac{1}{\sqrt{2\pi\sigma_0^2}}exp(-\frac{(\mu-\mu_0)^2}{2\sigma_0^2})\frac{\beta^{\alpha}}{\Gamma(\alpha)}(\frac{1}{\sigma^2})^{\alpha-1}exp(-\frac{\beta}{\sigma^2})
\end{align}

\begin{align}
L(\mu, \sigma)&=f(y_1,...,y_n|\mu, \sigma)\\
&=\prod_{i=1}^{n}[\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(y_i-\mu)^2}{2\sigma^2})]\\
&=(\frac{1}{\sqrt{2\pi\sigma^2}})^nexp(-\frac{\sum (y_i-\mu)^2}{2\sigma^2})
\end{align}

Since

\begin{align}
\pi(\mu, \sigma|y_1,...,y_n)\propto \pi(\mu, \sigma)L(\mu, \sigma)
\end{align}

so we have

\begin{align}
\pi(\mu|y_1,...,y_n,\sigma)\propto exp(-\frac{(\mu-\mu_0)^2}{2\sigma_0^2})exp(-\frac{\sum (y_i-\mu)^2}{2\sigma^2})
\end{align}

\begin{align}
\pi(\frac{1}{\sigma^2}|y_1,...,y_n,\mu)\propto (\frac{1}{\sigma^2})^{\alpha-1}exp(-\frac{\beta}{\sigma^2})(\frac{1}{\sigma^2})^{\frac n2}exp(-\frac{\sum (y_i-\mu)^2}{2\sigma^2})
\end{align}

In general, let's say we have 4 unknown parameters $\theta_1,\theta_2,\theta_3,\theta_4$ and the joint prior $\pi(\theta_1,\theta_2,\theta_3,\theta_4)$ and the likelihood $L(\theta_1,\theta_2,\theta_3,\theta_4)$. Then we have $\pi(\theta_1,\theta_2,\theta_3,\theta_4|y_1,...,y_n)\propto \pi(\theta_1,\theta_2,\theta_3,\theta_4)L(\theta_1,\theta_2,\theta_3,\theta_4)$.

And then, we get 

- $\pi(\theta_1|-)\propto...$
- $\pi(\theta_2|-)\propto...$
- $\pi(\theta_3|-)\propto...$
- $\pi(\theta_4|-)\propto...$


## Use R/RStudio to run a Gibbs sampler

```{r message = FALSE}
gibbs_normal <- function(input, S, seed){
  set.seed(seed)
  ybar <- mean(input$y)
  n <- length(input$y)
  para <- matrix(0, S, 2) # Sx2 zero matrix
  phi <- input$phi_init
  for(s in 1:S){
    mu1 <- (input$mu_0/input$sigma_0^2 + n*phi*ybar)/
    (1/input$sigma_0^2 + n*phi)
    sigma1 <- sqrt(1/(1/input$sigma_0^2 + n*phi))
    mu <- rnorm(1, mean = mu1, sd = sigma1)
    alpha1 <- input$alpha + n/2
    beta1 <- input$beta + sum((input$y - mu)^2)/2 
    phi <- rgamma(1, shape = alpha1, rate = beta1)
    para[s, ] <- c(mu, phi)
  }
  para }
```





- Run the Gibbs sampler:

\vspace{3mm}

```{r message = FALSE}
input <- list(y = CEsample$LogTotalExpLastQ, mu_0 = 5,sigma_0 = 1,
alpha = 1, beta = 1,phi_init = 1)
output <- gibbs_normal(input, S = 10000, seed = 123)
```

- Extract posterior draws of \texttt{mu} and \texttt{phi} from the Gibbs sampler output:

\vspace{3mm}

```{r message = FALSE}
para_post <- as.data.frame(output)
names(para_post) <- c("mu", "phi")
```


```{r fig.align = "center"}
ggplot(para_post, aes(mu)) + 
  geom_density(size = 2, color = crcblue) + 
  labs(title = "Posterior draws of mu") +
  theme_grey(base_size = 8, 
  base_family = "") 
```

```{r message = FALSE}
quantile(para_post$mu, c(0.025,0.975))
```


```{r fig.align = "center"}
ggplot(para_post, aes(phi)) + 
  geom_density(size = 2, color = crcblue) + 
  labs(title = "Posterior draws of phi") +
  theme_grey(base_size = 8, 
  base_family = "") 
```

```{r message = FALSE}
quantile(para_post$phi, c(0.025,0.975))
```


- Exercise 1: Update the Gibbs sampler to initiate it with `mu_init`. Compare your results to the existing results. You should get similar results meaning that it doesn't matter you start with $\mu$ or $\phi$ if indeed the Gibbs sampler has converted to the true posterior, you can even try different initial values.

```{r message = FALSE}
gibbs_normal <- function(input, S, seed){
  set.seed(seed)
  ybar <- mean(input$y)
  n <- length(input$y)
  para <- matrix(0, S, 2) # Sx2 zero matrix
  mu <- input$mu_init # change here
  for(s in 1:S){
    # start with phi first
    alpha1 <- input$alpha + n/2
    beta1 <- input$beta + sum((input$y - mu)^2)/2 
    phi <- rgamma(1, shape = alpha1, rate = beta1)
    
    mu1 <- (input$mu_0/input$sigma_0^2 + n*phi*ybar)/
    (1/input$sigma_0^2 + n*phi)
    sigma1 <- sqrt(1/(1/input$sigma_0^2 + n*phi))
    mu <- rnorm(1, mean = mu1, sd = sigma1)
    para[s, ] <- c(mu, phi)
  }
  para }
```


- Exercise 2: Update the Gibbs sampler to initiate it with `mu_init` and `phi_init`. Compare your results to the existing results.

If it converges, eventually the results should be the same.



# Use JAGS (Just Another Gibbs Sampler) and Bayesian inferences

## JAGS for unknown mean and standard deviation case 

- R package \texttt{runjags} to run Markov chain Monte Carlo simulations.

- Descriptive of the sampling model and the prior.


- Installing JAGS software and \texttt{runjags} R package
    - Download JAGS at [this link](https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/)
    - Install and load \texttt{runjags} R package
    
    \vspace{3mm}
    
```{r message = FALSE, eval = FALSE}
#install.packages("runjags")
```
    
```{r message = FALSE}
library(runjags)
```

- Only need to focus on the sampling density and the prior:
    - The sampling density: 
    \begin{equation}
    y_1, \cdots, y_n \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).      \nonumber
    \end{equation}
    
    - The prior distributions:
    \begin{eqnarray}
    \mu &\sim& \textrm{Normal}(\mu_0, \sigma_0), \nonumber \\
    1/\sigma^2 = \phi &\sim& \textrm{Gamma}(\alpha, \beta). \nonumber
    \end{eqnarray}

    
```{r message = FALSE}
modelString <- "
model{

# The sampling density
for (i in 1:N) {
y[i] ~ dnorm(mu, phi)
}

# The prior distributions
mu ~ dnorm(mu_0, phi_0)
phi ~ dgamma(alpha, beta)

}
"
```


- Pass the data and hyperparameter values to JAGS:

\vspace{3mm}

```{r message = FALSE}
y <- CEsample$LogTotalExpLastQ
N <- length(y)
the_data <- list("y" = y, "N" = N, "mu_0" = 5, "phi_0" = 1/1^2, 
"alpha" = 1,"beta" = 1)
```


- Run the JAGS code for this model:

\vspace{3mm}

```{r message = FALSE}
posterior <- run.jags(modelString,
                  data = the_data,
                  monitor = c("mu", "phi"),
                  n.chains = 1,
                  adapt = 1000,
                  burnin = 2000,
                  sample = 5000,
                  thin = 1)
```


- Obtain posterior summaries of \texttt{mu} and \texttt{phi}:

\vspace{3mm}

```{r message = FALSE}
summary(posterior) 
```

# MCMC diagnostics

## Trace plots example

```{r fig.align = "center"}
plot(posterior, vars = "mu")
```


## ACF plots example

```{r fig.align = "center"}
plot(posterior, vars = "mu")
```

## Effective sample size example

- The column of \texttt{SSeff}; recall \texttt{sample} is 5000.

\vspace{3mm}

```{r}
summary(posterior) 
```


## MCMC diagnostics for the CE example

```{r fig.align = "center"}
plot(posterior, vars = "mu")
```

## MCMC diagnostics for the CE example cont'd

```{r fig.align = "center"}
plot(posterior, vars = "phi")
```


## Gelman-Rubin diagnostics example

- Create intinial values of \texttt{mu} and \texttt{phi}:

\vspace{3mm}

```{r}
inits1 <- dump.format(list(mu = 1, phi = 1, 
			.RNG.name="base::Super-Duper", .RNG.seed = 1))
inits2 <- dump.format(list(mu = 10, phi = 10,
			.RNG.name="base::Wichmann-Hill", .RNG.seed = 2))
```


- Feed in \texttt{inits1} and \texttt{inits2}, and let \texttt{n.chains = 2}:

\vspace{3mm}

```{r}
posterior_2chains <- run.jags(modelString,
                      data = the_data,
                      monitor = c("mu", "phi"),
                      n.chains = 2,
                      inits=c(inits1, inits2), 
                      adapt = 1000,
                      burnin = 2000,
                      sample = 5000,
                      thin = 1)
```

## Gelman-Rubin diagnostics example cont'd

- Return \texttt{psrf} from the output, as Gelman-Rubin diagnostic results:

\vspace{3mm}

```{r}
posterior_2chains$psrf
```

## MCMC diagnostics for the CE example, 2 chains

```{r fig.align = "center"}
plot(posterior_2chains, vars = "mu")
```



## Useful diagnostics/functions in coda package

- One needs to convert parameter draws into an MCMC object. For example:

\vspace{3mm}

```{r message = FALSE, eval = FALSE}
# install.packages("coda")
```

```{r}
library(coda)
output <- gibbs_normal(input, S = 10000, seed = 123)
para_post = as.data.frame(output)
names(para_post) = c("mu", "phi")
```


- Then one can perform MCMC diagnostics. For example: 

\vspace{3mm}

```{r}
mu.mcmc = as.mcmc(para_post$mu)
```

```{r}
traceplot(mu.mcmc)
autocorr.plot(mu.mcmc)
effectiveSize(mu.mcmc)
```

```{r message = FALSE, eval = FALSE}
gelman.diag(mu.mcmc)
```

Note: \texttt{gelman.diag()} needs at least 2 chains.


# Recap

